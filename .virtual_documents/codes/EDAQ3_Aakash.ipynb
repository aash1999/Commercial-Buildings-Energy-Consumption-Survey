import numpy as np
import pandas as pd


data = pd.read_csv("../datasets/cbecs2018_final_public.csv")


data.shape


data.head()


data.columns


data.describe()


columns = ["DATACNTR", "TRNGRM", "STDNRM", "LGOFFDEV", "SMOFFDEV", "SERVER", "LAPTOP", "RFGSTO", "ELCOOL","GOVOWN", "PVC", \
           "PCTERMN", "LAPTPN", "TABLETN","SERVERN", "LGOFFDEVN", "SMOFFDEVN", "WBOARDSN", "COOLP", "DCNTRSFC", "ELEXP"]


df = data[columns]


df.head()


df.describe()


missing_values = df.isnull().sum()
print(missing_values)



df = df.dropna(subset=['ELEXP'])
df.shape


cat_cols = ["DATACNTR", "TRNGRM", "STDNRM", "LGOFFDEV", "SMOFFDEV", "SERVER", "LAPTOP", "RFGSTO", "ELCOOL","GOVOWN", "PVC", "DCNTRSFC" ]
cont_cols = ["PCTERMN", "LAPTPN", "TABLETN","SERVERN", "LGOFFDEVN", "SMOFFDEVN", "WBOARDSN", "COOLP", "ELEXP"]


unique_counts = df[cat_cols+cont_cols].nunique()
print(unique_counts)








import matplotlib.pyplot as plt


num_cols = len(cont_cols)

# Create subplots
rows = (num_cols + 1) // 2  # Calculate the number of rows needed
fig, axes = plt.subplots(rows, 2, figsize=(12, rows * 4))  # Adjust figure size
axes = axes.flatten()  # Flatten axes for easy iteration

# Plot histograms
for i, col in enumerate(cont_cols):
    axes[i].hist(df[col].dropna(), bins=40, alpha=0.7, color='blue')  # Exclude NA values
    axes[i].set_title(f'Histogram of {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Frequency')

# Hide unused subplots
for j in range(i + 1, len(axes)):
    axes[j].axis('off')

# Adjust layout
plt.tight_layout()
plt.show()



import seaborn as sns
import matplotlib.pyplot as plt

# Create a pair plot
sns.pairplot(df[cont_cols], diag_kind='kde', corner=True)  # 'diag_kind' for diagonal plots; 'corner=True' for a lower triangle plot
plt.show()



%matplotlib inline

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Compute the correlation matrix for numerical columns
correlation_matrix = df[cont_cols].corr()

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

# Plot the heatmap
plt.figure(figsize=(10, 8))  # Adjust the size as needed
sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", cbar=True, annot_kws={"size": 10})
plt.title("Correlation Matrix", fontsize=16)
plt.tight_layout()
plt.show()






import matplotlib.pyplot as plt



# Determine the number of rows and columns for subplots
n_cols = 2  # Number of pie charts per row
n_rows = (len(cat_cols) + n_cols - 1) // n_cols  # Calculate the number of rows

# Create a subplot grid
fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()  # Flatten the axes array for easy iteration

# Loop through each categorical column and plot the pie chart
for i, col in enumerate(cat_cols):
    value_counts = df[col].value_counts(normalize=True) * 100  # Percentage of unique values
    labels = value_counts.index  # Categories as labels
    sizes = value_counts.values  # Percentages as sizes

    axes[i].pie(sizes, labels=labels, autopct="%.1f%%", startangle=90)
    axes[i].set_title(f"Distribution of {col}")

# Remove any extra subplots (if cat_cols < total grid slots)
for j in range(len(cat_cols), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()






import seaborn as sns
import matplotlib.pyplot as plt

# Assume `cat_cols` contains the list of categorical columns
# Assume `num_col` is the numerical column for box plots
num_col = 'ELEXP'       # Replace with your actual numerical column

# Iterate through each categorical column
for cat_col in cat_cols:
    plt.figure(figsize=(10, 6))  # Set figure size
    sns.boxplot(x=cat_col, y=num_col, data=df)
    plt.title(f"Box Plot of {num_col} by {cat_col}")
    plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility
    plt.tight_layout()
    plt.show()



from scipy.stats import f_oneway

target_col = "ELEXP"
for cat_col in cat_cols:
    df1 = df.dropna(subset=[cat_col, target_col])
    # Group numerical target by each category in the categorical column
    groups = [df1[target_col][df1[cat_col] == category] for category in df1[cat_col].unique()]
    
    # Perform ANOVA
    f_stat, p_value = f_oneway(*groups)
    
    # Display Results
    print(f"ANOVA results for {cat_col}:")
    print(f"F-statistic: {f_stat:.4f}, P-value: {p_value:.4f}")
    print("-" * 40)



from scipy.stats import kruskal


for cat_col in cat_cols:
    # Group the target variable by each category in the categorical column
    df1 = df.dropna(subset=[cat_col, target_col])
    groups = [df1[target_col][df1[cat_col] == category] for category in df1[cat_col].unique()]
    
    # Perform Kruskal-Wallis Test
    h_stat, p_value = kruskal(*groups)
    
    # Display Results
    print(f"Kruskal-Wallis Test results for {cat_col}:")
    print(f"H-statistic: {h_stat:.4f}, P-value: {p_value:.4f}")
    print("-" * 40)






# cat_cols = ["DATACNTR", "TRNGRM", "STDNRM", "LGOFFDEV", "SMOFFDEV", "SERVER", "LAPTOP", "RFGSTO", "ELCOOL","GOVOWN"]
# cont_cols = ["PCTERMN", "LAPTPN", "TABLETN","SERVERN", "LGOFFDEVN", "SMOFFDEVN", "WBOARDSN", "COOLP", "ELEXP"]
# Replace NaN with median for all numerical columns
df = df[cont_cols+cat_cols]
df[cont_cols] = df[cont_cols].apply(lambda col: col.fillna(col.median()))

# Replace NaN with the mode (most frequent category) for categorical columns
df[cat_cols] = df[cat_cols].apply(lambda col: col.fillna(col.mode()[0]))






import pandas as pd

# One-hot encode categorical columns
df_data_prep = pd.get_dummies(df, columns=cat_cols, drop_first=True)  # drop_first=True avoids dummy variable trap

# Convert all columns to integers
df_data_prep = df_data_prep.astype(int)

print("DataFrame after converting all columns to integers:")
print(df_data_prep.info())



from sklearn.preprocessing import MinMaxScaler

# Select the columns you want to normalize (typically numerical columns)
columns_to_normalize = cont_cols  # Assuming `cont_cols` contains numerical column names

# Initialize MinMaxScaler with the desired feature range
scaler = MinMaxScaler(feature_range=(-1, 1))

# Normalize the selected columns
df_data_prep[columns_to_normalize] = scaler.fit_transform(df_data_prep[columns_to_normalize])

print("One-hot encoded DataFrame & Normalized DataFrame:") 
print(df_data_prep.head())
print(df_data_prep.columns)


import statsmodels.api as sm
import numpy as np
import pandas as pd



# Prepare the features (X) and target (y)
X = df_data_prep.drop(columns=[target_col])
y = df[target_col]

# Add an intercept (constant) to the model
X_encoded = sm.add_constant(X)

# Fit the model using statsmodels
model = sm.OLS(y, X_encoded)
results = model.fit()

# Print the summary of the model (which includes p-values)
print(results.summary())

# Extract the p-values for each feature and interaction term
p_values = results.pvalues
print(p_values)



X.shape


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


X = df_data_prep.drop(columns=[target_col])  # Features (independent variables)
y = df_data_prep[target_col]  # Target (dependent variable)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("Sahpe of X_train", X_train.shape)
print("Shape of X_test", X_test.shape)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.4f}")
print(f"R² Score: {r2:.4f}")

sns.residplot(x=y_test, y=y_pred, lowess=True, color="g", line_kws={'color': 'red'})
plt.title("Residual Plot")
plt.show()

plt.scatter(y_test, y_pred)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--r', lw=2)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Actual vs Predicted")
plt.show()



import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Define features and target
X = df_data_prep.drop(columns=[target_col])  # Features (independent variables)
y = df_data_prep[target_col]  # Target (dependent variable)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)

# Define models and their hyperparameter grids
models = {
    "Ridge": Ridge(),
    "Lasso": Lasso(),
    "ElasticNet": ElasticNet()
}

param_grids = {
    "Ridge": {"alpha": [0.01, 0.1, 1, 10, 100]},
    "Lasso": {"alpha": [0.01, 0.1, 1, 10, 100]},
    "ElasticNet": {
        "alpha": [0.01, 0.1, 1, 10, 100],
        "l1_ratio": [0.1, 0.5, 0.7, 1.0]
    }
}

best_models = {}
best_scores = {}

# Perform grid search for each model
for model_name, model in models.items():
    print(f"Tuning {model_name}...")
    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=5, scoring='r2', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_models[model_name] = grid_search.best_estimator_
    best_scores[model_name] = grid_search.best_score_
    print(f"Best {model_name} Parameters: {grid_search.best_params_}")
    print(f"Best {model_name} R² (CV): {grid_search.best_score_:.4f}")

# Evaluate the best model on the test set
best_model_name = max(best_scores, key=best_scores.get)
best_model = best_models[best_model_name]

print(f"\nBest Model: {best_model_name}")
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

# Calculate performance metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error (Test): {mse:.4f}")
print(f"R² Score (Test): {r2:.4f}")

# Residual plot
sns.residplot(x=y_test, y=y_pred, lowess=True, color="g", line_kws={'color': 'red'})
plt.title("Residual Plot")
plt.xlabel("Actual")
plt.ylabel("Residuals")
plt.show()

# Actual vs Predicted plot
plt.scatter(y_test, y_pred)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--r', lw=2)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Actual vs Predicted")
plt.show()



# Assume `cont_cols` is a list of continuous columns in your dataset
cont_cols_1 = cont_cols
cont_cols_1.remove("ELEXP")

# Create a new dataframe `df_poly` with squared terms for continuous columns
df_poly = df_data_prep.copy()
for col in cont_cols:
    if col != "ELEXP":
        df_poly[f"{col}^2"] = df_poly[col] ** 2

# Visualize relationships with a pair plot
sns.pairplot(df_poly, x_vars=cont_cols + [f"{col}^2" for col in cont_cols], y_vars=target_col, kind='scatter', diag_kind='kde', plot_kws={'alpha': 0.7})
plt.suptitle("Pair Plot of Features (Including Squared Terms) vs Target", y=1.02)
plt.show()

# Define features and target from `df_poly`
X_poly = df_poly.drop(columns=[target_col])
y_poly = df_poly[target_col]

# Split the data into training and testing sets
X_poly_train, X_poly_test, y_poly_train, y_poly_test = train_test_split(X_poly, y_poly, test_size=0.2, random_state=42)
print("Shape of X_poly_train:", X_poly_train.shape)
print("Shape of X_poly_test:", X_poly_test.shape)

# Train a Linear Regression model on the new dataset with squared terms
model_poly = LinearRegression()
model_poly.fit(X_poly_train, y_poly_train)

# Make predictions
y_poly_pred = model_poly.predict(X_poly_test)

# Evaluate the model
mse_poly = mean_squared_error(y_poly_test, y_poly_pred)
r2_poly = r2_score(y_poly_test, y_poly_pred)

print(f"Linear Regression with Squared Terms Performance:")
print(f"Mean Squared Error: {mse_poly:.4f}")
print(f"R² Score: {r2_poly:.4f}")

# Visualizations for the new model
sns.residplot(x=y_poly_test, y=y_poly_pred, lowess=True, color="b", line_kws={'color': 'red'})
plt.title("Residual Plot (Squared Terms)")
plt.show()

plt.scatter(y_poly_test, y_poly_pred)
plt.plot([min(y_poly_test), max(y_poly_test)], [min(y_poly_test), max(y_poly_test)], '--r', lw=2)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Actual vs Predicted (Squared Terms)")
plt.show()



df_data_prep[cont_cols]


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Ensure no negative values before log transformation
def safe_log_transform(series):
    """
    Applies log transformation safely by adding 1 to avoid log(0) and handling negative values.
    """
    return np.log1p(series)

# Plot histograms before and after log transformation
def plot_histograms_before_after_log(df, cont_cols):
    """
    Plots histograms for continuous features before and after log transformation.

    Args:
    df (pd.DataFrame): DataFrame containing the data.
    cont_cols (list): List of continuous columns to process.

    Returns:
    None
    """
    for col in cont_cols:
        # Original and log-transformed data
        original_data = df[col]
        transformed_data = safe_log_transform(df[col])

        # Plotting
        plt.figure(figsize=(14, 6))

        # Histogram of original data
        plt.subplot(1, 2, 1)
        sns.histplot(original_data, bins=60, kde=True, color="blue")
        plt.title(f"Histogram of {col} (Original)")
        plt.xlabel(col)
        plt.ylabel("Frequency")

        # Histogram of log-transformed data
        plt.subplot(1, 2, 2)
        sns.histplot(transformed_data, bins=60, kde=True, color="green")
        plt.title(f"Histogram of {col} (Log-Transformed)")
        plt.xlabel(f"Log-Transformed {col}")
        plt.ylabel("Frequency")

        plt.tight_layout()
        plt.show()

# Example usage
# Assuming df_data_prep is your DataFrame and cont_cols is your list of continuous columns

plot_histograms_before_after_log(df_data_prep, cont_cols)




