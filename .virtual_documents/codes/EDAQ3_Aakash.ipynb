import numpy as np
import pandas as pd


data = pd.read_csv("../datasets/cbecs2018_final_public.csv")


data.shape


data.head()


data.columns


data.describe()


columns = ["DATACNTR", "TRNGRM", "STDNRM", "LGOFFDEV", "SMOFFDEV", "SERVER", "LAPTOP", "RFGSTO", "ELCOOL","GOVOWN", "PVC", \
           "PCTERMN", "LAPTPN", "TABLETN","SERVERN", "LGOFFDEVN", "SMOFFDEVN", "WBOARDSN", "COOLP", "DCNTRSFC", "ELEXP"]


df = data[columns]


df.head()


df.describe()


missing_values = df.isnull().sum()
print(missing_values)



df = df.dropna(subset=['ELEXP'])
df.shape


cat_cols = ["DATACNTR", "TRNGRM", "STDNRM", "LGOFFDEV", "SMOFFDEV", "SERVER", "LAPTOP", "RFGSTO", "ELCOOL","GOVOWN", "PVC", "DCNTRSFC" ]
cont_cols = ["PCTERMN", "LAPTPN", "TABLETN","SERVERN", "LGOFFDEVN", "SMOFFDEVN", "WBOARDSN", "COOLP", "ELEXP"]


unique_counts = df[cat_cols+cont_cols].nunique()
print(unique_counts)








import matplotlib.pyplot as plt


num_cols = len(cont_cols)

# Create subplots
rows = (num_cols + 1) // 2  # Calculate the number of rows needed
fig, axes = plt.subplots(rows, 2, figsize=(12, rows * 4))  # Adjust figure size
axes = axes.flatten()  # Flatten axes for easy iteration

# Plot histograms
for i, col in enumerate(cont_cols):
    axes[i].hist(df[col].dropna(), bins=40, alpha=0.7, color='blue')  # Exclude NA values
    axes[i].set_title(f'Histogram of {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Frequency')

# Hide unused subplots
for j in range(i + 1, len(axes)):
    axes[j].axis('off')

# Adjust layout
plt.tight_layout()
plt.show()



import seaborn as sns
import matplotlib.pyplot as plt

# Create a pair plot
sns.pairplot(df[cont_cols], diag_kind='kde', corner=True)  # 'diag_kind' for diagonal plots; 'corner=True' for a lower triangle plot
plt.show()



%matplotlib inline

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Compute the correlation matrix for numerical columns
correlation_matrix = df[cont_cols].corr()

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

# Plot the heatmap
plt.figure(figsize=(10, 8))  # Adjust the size as needed
sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", cbar=True, annot_kws={"size": 10})
plt.title("Correlation Matrix", fontsize=16)
plt.tight_layout()
plt.show()






import matplotlib.pyplot as plt



# Determine the number of rows and columns for subplots
n_cols = 2  # Number of pie charts per row
n_rows = (len(cat_cols) + n_cols - 1) // n_cols  # Calculate the number of rows

# Create a subplot grid
fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()  # Flatten the axes array for easy iteration

# Loop through each categorical column and plot the pie chart
for i, col in enumerate(cat_cols):
    value_counts = df[col].value_counts(normalize=True) * 100  # Percentage of unique values
    labels = value_counts.index  # Categories as labels
    sizes = value_counts.values  # Percentages as sizes

    axes[i].pie(sizes, labels=labels, autopct="%.1f%%", startangle=90)
    axes[i].set_title(f"Distribution of {col}")

# Remove any extra subplots (if cat_cols < total grid slots)
for j in range(len(cat_cols), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()






import seaborn as sns
import matplotlib.pyplot as plt

# Assume `cat_cols` contains the list of categorical columns
# Assume `num_col` is the numerical column for box plots
num_col = 'ELEXP'       # Replace with your actual numerical column

# Iterate through each categorical column
for cat_col in cat_cols:
    plt.figure(figsize=(10, 6))  # Set figure size
    sns.boxplot(x=cat_col, y=num_col, data=df)
    plt.title(f"Box Plot of {num_col} by {cat_col}")
    plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility
    plt.tight_layout()
    plt.show()



from scipy.stats import f_oneway

target_col = "ELEXP"
for cat_col in cat_cols:
    df1 = df.dropna(subset=[cat_col, target_col])
    # Group numerical target by each category in the categorical column
    groups = [df1[target_col][df1[cat_col] == category] for category in df1[cat_col].unique()]
    
    # Perform ANOVA
    f_stat, p_value = f_oneway(*groups)
    
    # Display Results
    print(f"ANOVA results for {cat_col}:")
    print(f"F-statistic: {f_stat:.4f}, P-value: {p_value:.4f}")
    print("-" * 40)



from scipy.stats import kruskal


for cat_col in cat_cols:
    # Group the target variable by each category in the categorical column
    df1 = df.dropna(subset=[cat_col, target_col])
    groups = [df1[target_col][df1[cat_col] == category] for category in df1[cat_col].unique()]
    
    # Perform Kruskal-Wallis Test
    h_stat, p_value = kruskal(*groups)
    
    # Display Results
    print(f"Kruskal-Wallis Test results for {cat_col}:")
    print(f"H-statistic: {h_stat:.4f}, P-value: {p_value:.4f}")
    print("-" * 40)







# cat_cols = ["DATACNTR", "TRNGRM", "STDNRM", "LGOFFDEV", "SMOFFDEV", "SERVER", "LAPTOP", "RFGSTO", "ELCOOL","GOVOWN"]
# cont_cols = ["PCTERMN", "LAPTPN", "TABLETN","SERVERN", "LGOFFDEVN", "SMOFFDEVN", "WBOARDSN", "COOLP", "ELEXP"]
# Replace NaN with median for all numerical columns
df_imputed_0 = df[cont_cols+cat_cols]
df_imputed_0[cont_cols] = df_imputed_0[cont_cols].apply(lambda col: col.fillna(col.median()))

# Replace NaN with the mode (most frequent category) for categorical columns
df_imputed_0[cat_cols] = df_imputed_0[cat_cols].apply(lambda col: col.fillna(col.mode()[0]))


from sklearn.impute import KNNImputer
import pandas as pd

# Assuming df is the DataFrame and cont_cols, cat_cols are already defined

# Create a copy of the dataframe to avoid modifying the original one
df_imputed = df.copy()

# Separate continuous and categorical columns
X_continuous = df_imputed[cont_cols]
X_categorical = df_imputed[cat_cols]

# KNN Imputation for Continuous Columns
knn_imputer = KNNImputer(n_neighbors=5)
df_imputed[cont_cols] = knn_imputer.fit_transform(X_continuous)

# KNN Imputation for Categorical Columns (using the mode for categorical features)
# First, convert categorical columns to numeric for KNN imputation (using label encoding)
X_categorical_encoded = X_categorical.apply(lambda col: col.astype('category').cat.codes)

# Perform KNN imputation for categorical data
df_imputed[cat_cols] = knn_imputer.fit_transform(X_categorical_encoded)

# Convert back to original categorical values (decode the encoded labels)
df_imputed[cat_cols] = df_imputed[cat_cols].apply(lambda col: col.astype('category').cat.codes)
for col in cat_cols:
    df_imputed[col] = df_imputed[col].astype('category')

# Output the imputed dataframe
print(df_imputed.head())






import pandas as pd

# One-hot encode categorical columns
df_data_prep = pd.get_dummies(df_imputed, columns=cat_cols, drop_first=True)  # drop_first=True avoids dummy variable trap

# Convert all columns to integers
df_data_prep = df_data_prep.astype(int)

print("DataFrame after converting all columns to integers:")
print(df_data_prep.info())



from sklearn.preprocessing import MinMaxScaler

# Select the columns you want to normalize (typically numerical columns)
columns_to_normalize = cont_cols  # Assuming `cont_cols` contains numerical column names

# Initialize MinMaxScaler with the desired feature range
scaler = MinMaxScaler(feature_range=(-1, 1))

# Normalize the selected columns
df_data_prep[columns_to_normalize] = scaler.fit_transform(df_data_prep[columns_to_normalize])

print("One-hot encoded DataFrame & Normalized DataFrame:") 
print(df_data_prep.head())
print(df_data_prep.columns)


import statsmodels.api as sm
import numpy as np
import pandas as pd



# Prepare the features (X) and target (y)
X = df_data_prep.drop(columns=[target_col])
y = df[target_col]

# Add an intercept (constant) to the model
X_encoded = sm.add_constant(X)

# Fit the model using statsmodels
model = sm.OLS(y, X_encoded)
results = model.fit()

# Print the summary of the model (which includes p-values)
print(results.summary())

# Extract the p-values for each feature and interaction term
p_values = results.pvalues
print(p_values)

# Extract features with p-value less than 0.05
significant_features = p_values[p_values < 0.05].index.tolist()

# Exclude the constant term from the list of features
significant_features = [feature for feature in significant_features if feature != 'const']

# Print the significant features
print("Significant Features (p-value < 0.05):")
print(significant_features)
print(len(significant_features))




df_data_prep1 = df_data_prep[significant_features+[target_col]]
df_data_prep1.head()


import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Assuming significant_features is the list of features with p-value < 0.05
X_significant = df_data_prep1[significant_features]  # Use only significant features
y = df_data_prep1[target_col]  # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_significant, y, test_size=0.2, random_state=42)

# Initialize the Linear Regression model
lr_model = LinearRegression()

# List to store the training loss (MSE) for each epoch
epochs = 100  # Number of epochs
training_loss = []

# Train the model over multiple epochs
for epoch in range(epochs):
    # Fit the model on the training data
    lr_model.fit(X_train, y_train)
    
    # Predict on the training data
    y_train_pred = lr_model.predict(X_train)
    
    # Calculate MSE for the entire training data and store it
    mse_train = mean_squared_error(y_train, y_train_pred)
    training_loss.append(mse_train)
    
    # Optionally print progress for every 10 epochs
    if (epoch + 1) % 10 == 0:
        print(f"Epoch {epoch + 1}/{epochs}, Training MSE: {mse_train:.4f}")

# Predict on the test set
y_train_pred = lr_model.predict(X_train)
y_test_pred = lr_model.predict(X_test)

# Evaluate the model
mse_train = mean_squared_error(y_train, y_train_pred)
mse_test = mean_squared_error(y_test, y_test_pred)
r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)

# Print the evaluation metrics
print(f"Training Mean Squared Error: {mse_train:.4f}")
print(f"Test Mean Squared Error: {mse_test:.4f}")
print(f"Training R-squared: {r2_train:.4f}")
print(f"Test R-squared: {r2_test:.4f}")

# Plot 1: Loss over training (MSE over epochs)
plt.figure(figsize=(10, 6))
plt.plot(range(1, epochs + 1), training_loss, color='b', label='Training Loss (MSE)')
plt.title("Training Loss Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Mean Squared Error (MSE)")
plt.legend()
plt.show()

# Plot 2: Actual vs Predicted (y_pred vs y_test)
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_test_pred, color='r', alpha=0.6)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='black', linestyle='--', label='Perfect Prediction')
plt.title("Actual vs Predicted")
plt.xlabel("Actual Values (y_test)")
plt.ylabel("Predicted Values (y_pred)")
plt.legend()
plt.show()




