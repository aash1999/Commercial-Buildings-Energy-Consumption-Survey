import numpy as np
import pandas as pd


data = pd.read_csv("../datasets/cbecs2018_final_public.csv")


data.head()


data.columns


data.describe()


columns = ["DATACNTR", "TRNGRM", "STDNRM", "LGOFFDEV", "SMOFFDEV", "SERVER", "LAPTOP", "RFGSTO", "ELCOOL","GOVOWN", "PVC", \
           "PCTERMN", "LAPTPN", "TABLETN","SERVERN", "LGOFFDEVN", "SMOFFDEVN", "WBOARDSN", "COOLP", "DCNTRSFC", "ELEXP"]


df = data[columns]


df.head()


df.describe()


missing_values = df.isnull().sum()
print(missing_values)



df = df.dropna(subset=['ELEXP'])


cat_cols = ["DATACNTR", "TRNGRM", "STDNRM", "LGOFFDEV", "SMOFFDEV", "SERVER", "LAPTOP", "RFGSTO", "ELCOOL","GOVOWN", "PVC", ]
cont_cols = ["PCTERMN", "LAPTPN", "TABLETN","SERVERN", "LGOFFDEVN", "SMOFFDEVN", "WBOARDSN", "COOLP", "DCNTRSFC", "ELEXP"]


unique_counts = df[cat_cols].nunique()
print(unique_counts)





for col in cont_cols:
    Q1 = df[col].quantile(0.25)  # 25th percentile
    Q3 = df[col].quantile(0.75)  # 75th percentile
    IQR = Q3 - Q1  # Interquartile range
    
    # Define the lower and upper bounds
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Remove rows outside the bounds
    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]


import matplotlib.pyplot as plt


num_cols = len(cont_cols)

# Create subplots
rows = (num_cols + 1) // 2  # Calculate the number of rows needed
fig, axes = plt.subplots(rows, 2, figsize=(12, rows * 4))  # Adjust figure size
axes = axes.flatten()  # Flatten axes for easy iteration

# Plot histograms
for i, col in enumerate(cont_cols):
    axes[i].hist(df[col].dropna(), bins=20, alpha=0.7, color='blue')  # Exclude NA values
    axes[i].set_title(f'Histogram of {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Frequency')

# Hide unused subplots
for j in range(i + 1, len(axes)):
    axes[j].axis('off')

# Adjust layout
plt.tight_layout()
plt.show()



import seaborn as sns
import matplotlib.pyplot as plt

# Create a pair plot
sns.pairplot(df[cont_cols], diag_kind='kde', corner=True)  # 'diag_kind' for diagonal plots; 'corner=True' for a lower triangle plot
plt.show()



%matplotlib inline

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Compute the correlation matrix for numerical columns
correlation_matrix = df[cont_cols].corr()

# Create a mask for the upper triangle
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

# Plot the heatmap
plt.figure(figsize=(10, 8))  # Adjust the size as needed
sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt=".2f", cmap="coolwarm", cbar=True, annot_kws={"size": 10})
plt.title("Correlation Matrix", fontsize=16)
plt.tight_layout()
plt.show()






import matplotlib.pyplot as plt



# Determine the number of rows and columns for subplots
n_cols = 2  # Number of pie charts per row
n_rows = (len(cat_cols) + n_cols - 1) // n_cols  # Calculate the number of rows

# Create a subplot grid
fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()  # Flatten the axes array for easy iteration

# Loop through each categorical column and plot the pie chart
for i, col in enumerate(cat_cols):
    value_counts = df[col].value_counts(normalize=True) * 100  # Percentage of unique values
    labels = value_counts.index  # Categories as labels
    sizes = value_counts.values  # Percentages as sizes

    axes[i].pie(sizes, labels=labels, autopct="%.1f%%", startangle=90)
    axes[i].set_title(f"Distribution of {col}")

# Remove any extra subplots (if cat_cols < total grid slots)
for j in range(len(cat_cols), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()



cat_cols = ["DATACNTR", "TRNGRM", "STDNRM", "RFGSTO", "ELCOOL","GOVOWN"]


import seaborn as sns
import matplotlib.pyplot as plt

# Assume `cat_cols` contains the list of categorical columns
# Assume `num_col` is the numerical column for box plots
num_col = 'ELEXP'       # Replace with your actual numerical column

# Iterate through each categorical column
for cat_col in cat_cols:
    plt.figure(figsize=(10, 6))  # Set figure size
    sns.boxplot(x=cat_col, y=num_col, data=df)
    plt.title(f"Box Plot of {num_col} by {cat_col}")
    plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility
    plt.tight_layout()
    plt.show()



from scipy.stats import f_oneway

target_col = "ELEXP"
for cat_col in cat_cols:
    # Group numerical target by each category in the categorical column
    groups = [df[target_col][df[cat_col] == category] for category in df[cat_col].unique()]
    
    # Perform ANOVA
    f_stat, p_value = f_oneway(*groups)
    
    # Display Results
    print(f"ANOVA results for {cat_col}:")
    print(f"F-statistic: {f_stat:.4f}, P-value: {p_value:.4f}")
    print("-" * 40)



from scipy.stats import kruskal


for cat_col in cat_cols:
    # Group the target variable by each category in the categorical column
    groups = [df[target_col][df[cat_col] == category] for category in df[cat_col].unique()]
    
    # Perform Kruskal-Wallis Test
    h_stat, p_value = kruskal(*groups)
    
    # Display Results
    print(f"Kruskal-Wallis Test results for {cat_col}:")
    print(f"H-statistic: {h_stat:.4f}, P-value: {p_value:.4f}")
    print("-" * 40)






# cat_cols = ["DATACNTR", "TRNGRM", "STDNRM", "LGOFFDEV", "SMOFFDEV", "SERVER", "LAPTOP", "RFGSTO", "ELCOOL","GOVOWN"]
# cont_cols = ["PCTERMN", "LAPTPN", "TABLETN","SERVERN", "LGOFFDEVN", "SMOFFDEVN", "WBOARDSN", "COOLP", "ELEXP"]
# Replace NaN with median for all numerical columns
df[cont_cols] = df[cont_cols].apply(lambda col: col.fillna(col.median()))

# Replace NaN with the mode (most frequent category) for categorical columns
df[cat_cols] = df[cat_cols].apply(lambda col: col.fillna(col.mode()[0]))






import statsmodels.api as sm
import numpy as np
import pandas as pd

# Assuming you already have df, cont_cols, cat_cols, and target_col defined

# Prepare the features (X) and target (y)
X = df[cont_cols + cat_cols].drop(columns=[target_col])
y = df[target_col]

# Encode categorical variables if necessary (using one-hot encoding, for example)
X_encoded = pd.get_dummies(X, drop_first=True)  # Create dummy variables for categorical features

# Add an intercept (constant) to the model
X_encoded = sm.add_constant(X_encoded)

# Fit the model using statsmodels
model = sm.OLS(y, X_encoded)
results = model.fit()

# Print the summary of the model (which includes p-values)
print(results.summary())

# Extract the p-values for each feature and interaction term
p_values = results.pvalues
print(p_values)



import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

X = df[cont_cols+cat_cols].drop(columns=[target_col])  # Features (independent variables)
y = df[target_col]  # Target (dependent variable)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.4f}")
print(f"RÂ² Score: {r2:.4f}")

sns.residplot(x=y_test, y=y_pred, lowess=True, color="g", line_kws={'color': 'red'})
plt.title("Residual Plot")
plt.show()

plt.scatter(y_test, y_pred)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--r', lw=2)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Actual vs Predicted")
plt.show()




