# Importing necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

# Load the dataset
df = pd.read_csv('/content/cbecs2018_final_public.csv')

# Define the target variable
target_variable = 'ELCNS'

# Ensure the target variable exists
if target_variable not in df.columns:
    raise ValueError(f"Target variable '{target_variable}' not found in the dataset.")

# Handle missing values in the target variable
df[target_variable] = pd.to_numeric(df[target_variable].fillna(df[target_variable].mean()))

# Remove unnecessary columns based on filtering criteria
df = df.loc[:, ~df.columns.str.startswith(('Z', 'FINAL', 'AIRHAND', 'BLR', 'DH', 'RFC', 'CW', 'M', 'LT'))]
df = df.loc[:, ~df.columns.str.contains('(OTH|FK|REN|HW|NG|WTR|RFG|PK|FA|TR|COOL|HEATP)', case=False, regex=True)]
df = df.loc[:, ~df.columns.str.endswith(('PC', 'SEAT', 'BED', 'HT', 'PR', 'WO', 'OT', 'USED', 'BTU', '1', '2'))]

# Remove columns with more than 20% missing values
missing_percentage = (df.isnull().sum() / len(df)) * 100
df = df.loc[:, missing_percentage <= 20]

# Use SimpleImputer to fill missing values for all features
imputer = SimpleImputer(strategy='mean')  # Or use 'median' if appropriate for specific columns
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# Separate features and target
X = df_imputed.drop(columns=[target_variable])
y = df_imputed[target_variable]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit a Random Forest model to calculate feature importance
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Extract feature importances
feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf.feature_importances_
}).sort_values(by='Importance', ascending=False)

# Select columns with importance > 0.01
selected_features = feature_importances[feature_importances['Importance'] > 0.01]['Feature'].tolist()
X_selected = X[selected_features]

# Scale the selected features for clustering
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_selected)

# Apply K-means clustering
kmeans = KMeans(n_clusters=3, random_state=42)  # Example with 3 clusters
df_imputed['Cluster'] = kmeans.fit_predict(X_scaled)

# Analyze characteristics of each cluster
cluster_summary = df_imputed.groupby('Cluster').mean()

# Print cluster characteristics
print("Cluster Characteristics:")
print(cluster_summary)

# Visualize clusters
sns.boxplot(data=df_imputed, x='Cluster', y=target_variable)
plt.title("Electricity Consumption by Cluster")
plt.xlabel("Cluster")
plt.ylabel("Annual Electricity Consumption (2018)")
plt.show()

# Save clustered data for further analysis
df_imputed.to_csv('clustered_buildings.csv', index=False)
